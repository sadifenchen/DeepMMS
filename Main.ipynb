{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Optional\n",
    "import numpy as np\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn \n",
    "from flax.training import train_state \n",
    "from flax.linen.initializers import zeros as nn_zeros\n",
    "import optax \n",
    "import pymbar \n",
    "import sys \n",
    "\n",
    "import pickle\n",
    "\n",
    "from flax.linen.initializers import lecun_normal\n",
    "\n",
    "import libs.tool_box as TB\n",
    "import jax_amber\n",
    "\n",
    "default_kernel_init = lecun_normal()\n",
    "    ### This function initialize the WEIGHTS of neural network.\n",
    "    #   generate random weights with values drawn from a Guassian distribution\n",
    "    #   with mean = 0 and standard deviation of 1/sqrt(n) \n",
    "    ### where n is the number of input neurons.\n",
    "\n",
    "RT = jnp.float32(8.3144621E-3 * 300.0) \n",
    "beta = jnp.float32(1.0)/RT \n",
    "nm2ang = jnp.float32(10.0) #conversion nanometers -> angstroms\n",
    "ang2nm = jnp.float32(0.1) #conversion angstroms -> nanometers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deddfbd",
   "metadata": {},
   "source": [
    "# Load setting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b030640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_json = 'data/F18/input_test.json'\n",
    "with open(fname_json) as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a809460",
   "metadata": {},
   "source": [
    "# Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc768d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### main training of the model\n",
    "#   read from an input json file\n",
    "###\n",
    "fout = open (json_data['fname_log'], 'w', 1)\n",
    "\n",
    "\n",
    "# x = training data\n",
    "# tx = testing data\n",
    "x_A, tx_A = TB.get_trajectory (json_data['fname_prmtop'],\n",
    "                      json_data['fname_dcd_A'],\n",
    "                      json_data['nsamp'])\n",
    "x_B, tx_B = TB.get_trajectory (json_data['fname_prmtop'],\n",
    "                      json_data['fname_dcd_B'],\n",
    "                      json_data['nsamp'])\n",
    "\n",
    "\n",
    "# Combine data into a tuple set\n",
    "inputs = (x_A, x_B)\n",
    "\n",
    "# Get number of configurations in the training data\n",
    "nconf = x_A.shape[0]\n",
    "\n",
    "# Extract fixed atom indices and reference positions for the restraint\n",
    "fixed_atoms = jnp.array (json_data['fixed']['atoms']) - 1\n",
    "R0_A = jnp.array (json_data['fixed']['R0_A'])\n",
    "R0_B = jnp.array (json_data['fixed']['R0_B'])\n",
    "#dF0  = jnp.float32 (json_data['fixed']['dF0']) ### previous testing line\n",
    "kval = jnp.float32 (json_data['fixed']['kval'])\n",
    "dR0_AB = R0_B - R0_A\n",
    "d_lam = json_data['d_lambda']\n",
    "\n",
    "# Extract the index of the atom to which the restraint is applied\n",
    "fixed_iatom = fixed_atoms[-1]\n",
    "\n",
    "### Uses jax_amber.py file to calculate energy function\n",
    "ener_funs = jax_amber.get_amber_energy_funs (json_data['fname_prmtop'],\n",
    "                                            fixed_iatom,\n",
    "                                            kval)\n",
    "ener_nHO_fun, ener_wHO_fun, ener_bond_fun = ener_funs \n",
    "\n",
    "# Compute the reference energies for each system and each energy component\n",
    "enr_bnd_A0, enr_nHO_A0, enr_wHO_A0 = TB.get_energy_values (x_A, ener_funs, R0_A)\n",
    "enr_bnd_B0, enr_nHO_B0, enr_wHO_B0 = TB.get_energy_values (x_B, ener_funs, R0_B)\n",
    "###(TESTING)\n",
    "_, _, enr_wHO_A0_test = TB.get_energy_values (tx_A, ener_funs, R0_A)\n",
    "_, _, enr_wHO_B0_test = TB.get_energy_values (tx_B, ener_funs, R0_B)\n",
    "###\n",
    "\n",
    "# Calculate energy difference, average fixed Z-coordinates and bond energy\n",
    "dE0 = (enr_wHO_B0-enr_wHO_A0).mean()\n",
    "Z_A = x_A[:,fixed_iatom,2].mean()\n",
    "Z_B = x_B[:,fixed_iatom,2].mean()\n",
    "enr_bnd_A0 = enr_bnd_A0.mean()\n",
    "enr_bnd_B0 = enr_bnd_B0.mean()\n",
    "\n",
    "# Print output to log file\n",
    "print (' Fixed_Z:           {:12.6f} {:12.6f}'.format(Z_A, Z_B), file=fout)\n",
    "print (' <U_wHO0>(kJ/mol):  {:12.6f} {:12.6f}'.format (enr_wHO_A0.mean(), enr_wHO_B0.mean()), file=fout)\n",
    "print (' <U_nHO0>(kJ/mol):  {:12.6f} {:12.6f}'.format (enr_nHO_A0.mean(), enr_nHO_B0.mean()), file=fout)\n",
    "print ('<dU>[w/no](kJ/mol): {:12.6f} {:12.6f}'.format( dE0, (enr_nHO_B0-enr_nHO_A0).mean() ), file=fout)\n",
    "print ('<enr_bond>(kJ/mol): {:12.6f} {:12.6f}'.format(enr_bnd_A0, enr_bnd_B0), file=fout)\n",
    "\n",
    "# Save energy values for reference and testing\n",
    "ener_ref0 = (enr_nHO_A0, enr_nHO_B0), \\\n",
    "        (enr_wHO_A0, enr_wHO_B0, enr_bnd_A0, enr_bnd_B0 )\n",
    "ener_wHO_ref0_test = (enr_wHO_A0_test, enr_wHO_B0_test, enr_bnd_A0, enr_bnd_B0 )\n",
    "\n",
    "# Set optimization parameters   \n",
    "lr = json_data['optax']['learning_rate']\n",
    "total_steps = json_data['optax']['total_steps']\n",
    "alpha = json_data['optax']['alpha']\n",
    "scheduler = optax.cosine_decay_schedule (lr, \n",
    "                                        decay_steps=total_steps,\n",
    "                                        alpha=alpha)\n",
    "opt_method = optax.adam (learning_rate=scheduler)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, key = jax.random.split (rng)\n",
    "\n",
    "# Set RealNVP parameters\n",
    "input_size = x_A.shape[1]*3\n",
    "hidden_dim = json_data['realNVP']['hidden_dim']\n",
    "hidden_layers=json_data['realNVP']['hidden_layers']\n",
    "mask_fixed = jnp.array(json_data['realNVP']['mask_fixed']) - 1\n",
    "model = realNVP3(input_size=input_size, \n",
    "                 hidden_layers=hidden_layers,\n",
    "                 hidden_dim=hidden_dim,\n",
    "                 fixed_atoms=mask_fixed)\n",
    "\n",
    "# Create the initial training state\n",
    "state = train_state.TrainState.create (\n",
    "    apply_fn=model.apply,\n",
    "    params=model.init (key, x_A)['params'],\n",
    "    tx=opt_method\n",
    ")\n",
    "\n",
    "\n",
    "lam_max = jnp.float32(1.0)\n",
    "lam = lam_max\n",
    "test_ckpt = {'params': state.params, \n",
    "            'opt_state':state.opt_state}\n",
    "\n",
    "### Depends on the input json data\n",
    "#   If the restart_nn option is enabled, \n",
    "### Load the neural network from the checkpoint file\n",
    "if json_data['restart_nn']['run']:\n",
    "    ckpt = checkpoint_load (json_data['restart_nn']['fname_nn_pkl'])\n",
    "\n",
    "    state = state.replace (step=state.step, \n",
    "                            params=ckpt['params'], \n",
    "                            opt_state=ckpt['opt_state'])\n",
    "    lam = ckpt['lam']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1219cd7",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step (state, inputs, ener_wHO_ref0, fixed_R0):\n",
    "    # Train a step for the model\n",
    "    def loss_fn (params, apply_fn):\n",
    "        # Calculate loss value\n",
    "        x_A, x_B = inputs\n",
    "\n",
    "        m_B, log_J_F = apply_fn ({'params':params}, x_A)\n",
    "        m_A, log_J_R = apply_fn ({'params':params}, x_B, reverse=True)\n",
    "\n",
    "        loss_wBnd, loss = TB.loss_value (ener_wHO_fun, ener_bond_fun, ener_wHO_ref0,\n",
    "            m_B, log_J_F, m_A, log_J_R, fixed_R0)\n",
    "\n",
    "        return loss_wBnd\n",
    "\n",
    "    grads = jax.grad (loss_fn) (state.params, state.apply_fn)\n",
    "\n",
    "    return state.apply_gradients (grads=grads)\n",
    "\n",
    "\n",
    "R_A = R0_B - lam*dR0_AB \n",
    "R_B = R0_A + lam*dR0_AB\n",
    "dE  = lam*dE0\n",
    "fixed_R0 = (R_A, R_B, dE)\n",
    "\n",
    "loss_old = 0.0\n",
    "loss_test_min = 1000.0\n",
    "loss_test_list = []\n",
    "\n",
    "for epoch in range (json_data['nepoch']):\n",
    "\n",
    "    # loop over batches\n",
    "    for ist0 in range (0,nconf,1000):\n",
    "        ied0 = ist0 + 1000\n",
    "        ied0 = jnp.where (ied0 < nconf, ied0, nconf)\n",
    "        batch = (x_A[ist0:ied0], x_B[ist0:ied0])\n",
    "        ener_wHO_ref0 = (enr_wHO_A0[ist0:ied0], enr_wHO_B0[ist0:ied0], \\\n",
    "                enr_bnd_A0, enr_bnd_B0)\n",
    "\n",
    "        state = train_step (state, batch, ener_wHO_ref0, fixed_R0)\n",
    "\n",
    "    # every 10 epochs print the loss\n",
    "    if (epoch+1)%10 == 0:\n",
    "        m_B, log_J_F = state.apply_fn ({'params':state.params}, x_A)\n",
    "        m_A, log_J_R = state.apply_fn ({'params':state.params}, x_B, reverse=True)\n",
    "\n",
    "        loss_Wbnd, loss = TB.loss_value (ener_wHO_fun, ener_bond_fun, ener_ref0[1],\n",
    "                         m_B, log_J_F, m_A, log_J_R, fixed_R0)\n",
    "        diff = loss_Wbnd - loss_old \n",
    "        loss_old = loss_Wbnd\n",
    "\n",
    "        m_B, log_J_F = state.apply_fn ({'params':state.params}, tx_A)\n",
    "        m_A, log_J_R = state.apply_fn ({'params':state.params}, tx_B, reverse=True)\n",
    "        _, loss_test = TB.loss_value (ener_wHO_fun, ener_bond_fun, ener_wHO_ref0_test,\n",
    "                         m_B, log_J_F, m_A, log_J_R, fixed_R0)\n",
    "        print ('loss {:8d} {:12.4f} {:12.4f} {:12.4f} {:14.4f}'.format(\n",
    "            epoch+1, loss, loss_Wbnd, diff, loss_test),file=fout)\n",
    "\n",
    "        # save the state if the loss on the test set is reduced\n",
    "        if loss_test < loss_test_min:\n",
    "            loss_test_min = loss_test \n",
    "            test_ckpt = {'params': state.params, \n",
    "                        'opt_state':state.opt_state}\n",
    "\n",
    "        # break if the loss is less than 0\n",
    "        if loss < jnp.float32(0.0):\n",
    "            break \n",
    "\n",
    "        loss_test_list.append (loss_test)\n",
    "\n",
    "    # every 200 epochs print progress\n",
    "    if (epoch+1)%200 == 0:\n",
    "        TB.print_progress (state, inputs,\n",
    "                        ener_funs,\n",
    "                        ener_ref0,\n",
    "                        fixed_iatom,\n",
    "                        fixed_R0, fout)\n",
    "        loss_test = jnp.array (loss_test_list).min()\n",
    "\n",
    "        # break condition\n",
    "        if loss_test > loss_test_min + 3.0:\n",
    "            print ('loss_test_min', loss_test_min, file=fout)\n",
    "            break\n",
    "        loss_test_list = [] # clear the loss test list for next epochs\n",
    "\n",
    "\n",
    "ckpt = {'params': state.params, 'opt_state': state.opt_state, 'lam': lam}\n",
    "checkpoint_save (json_data['fname_nn_pkl'], ckpt)\n",
    "\n",
    "# Save the current state of the model with the best test loss so far\n",
    "checkpoint_save (json_data['fname_nn_test_pkl'], test_ckpt)\n",
    "\n",
    "# Update the state of the model with the saved state\n",
    "state = state.replace (step=0,\n",
    "                        params=test_ckpt['params'],\n",
    "                        opt_state=test_ckpt['opt_state'])\n",
    "\n",
    "print (\"===SUMMARY===\", file=fout)\n",
    "\n",
    "# Print the progress ~\n",
    "print_progress (state, inputs,\n",
    "                        ener_funs,\n",
    "                        ener_ref0,\n",
    "                        fixed_iatom,\n",
    "                        fixed_R0, fout)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
